\begin{framecard}
	{\color{colorbg}
	\bfseries

	\hugetext{Open games with agency}}
\end{framecard}

\begin{frame}{What is a game?}
	\begin{quotation}
		\centering
		Game theory is the mathematical study of {\color<2->{colorarena}interaction}\\ among independent, self-interested {\color<2->{coloragents}agents}.\\
		{\color{colornote}-- Essentials of Game Theory, \cite{leyton2008essentials}}
	\end{quotation}

	\vfill
	\begin{center}
		\alt<2->{
			\includegraphics[width=.5\textwidth]{figures/board_games2.jpg}
		}{
			\includegraphics[width=.5\textwidth]{figures/board-games.png}
		}
	\end{center}

	\vfill
	\onslide<2->{
		A game factors in two parts:
		\begin{enumerate}
			\item An \textcolor{colorarena}{\textbf{arena}}, which models the \textbf{dynamics} of the game.
			\item Some \textcolor{coloragents}{\textbf{players}}, which intervene in the arena by making \textbf{decisions}.
		\end{enumerate}
	}

	\vfill
	\onslide<3->{
		A \textcolor{coloragents}{strategy} $\colag{\omega \in \Omega_p}$ for a player $\colag{1 \leq p \leq N}$ is a policy $\colag p$ uses to make their decisions (e.g. a choice of move for each of $\colag p$'s rounds).

		\vfill
		A \textcolor{coloragents}{strategy profile} is a strategy for each player $\colag{\Omega = \Omega_1 \times \cdots \times \Omega_N}$.
	}
\end{frame}

\begin{frame}{What is an arena?}
	An \textcolor{colorarena}{arena} is an open system with three boundaries:

	\begin{center}
		\includegraphics[clip, page=2, trim=0cm 3cm 3cm 1.5cm, width=.8\textwidth]{figures/drawings.pdf}
	\end{center}

	This is a \textbf{parametrised lens} $\textcolor{colorarena}{\A : (X,S) \nlongto{\textcolor{coloragents}{(\Omega, \Comega)}} (Y,R)}$ specified by two maps
	\begin{eqalign*}
		\textcolor{colorarena}{\play} &: \textcolor{coloragents}{\Omega} \textcolor{colorarena}{\times X \to Y},\\
		\textcolor{colorarena}{\coplay} &: \textcolor{coloragents}{\Omega} \textcolor{colorarena}{\times X \times R \to \textcolor{coloragents}{\Comega} \times S}
	\end{eqalign*}
\end{frame}

\begin{frame}{What is an arena?}
	\begin{center}
		It can be `closed' by specifying an \textcolor{colorarena}{\textbf{initial state}} and a \textcolor{colorarena}{\textbf{utility function}}:
	\end{center}

	\begin{center}
		\includegraphics[clip, page=4, trim=4.5cm 3cm 2cm 7cm, width=.9\textwidth]{figures/drawings.pdf}
	\end{center}
	Observe that $\colar{\Lens(1,1)(X,S) \iso X}$ and $\colar{\Lens(Y,R)(1,1) \iso Y \to R}$.
\end{frame}

\begin{frame}{What is an arena?}
	\begin{center}
		A \textcolor{colorarena}{\textbf{closed arena}} amounts to an evaluation of strategies with rewards:
	\end{center}

	\vspace{5ex}
	\begin{center}
		\includegraphics[clip, page=5, trim=3cm 6.5cm .5cm 5cm, width=.9\textwidth]{figures/drawings.pdf}
	\end{center}
\end{frame}

\begin{frame}{What are players?}
	\begin{center}
		\textcolor{coloragents}{Players} are a \emph{distinct part} of the game (seen as a \textbf{system})\\
		which expresses \textcolor{coloragents}{preferences} by means of a \textcolor{coloragents}{\textbf{selection function}}:
	\end{center}

	{\fontsize{1.5em}{1.5em}\selectfont
	\begin{equation*}
		\colag{\varepsilon} : (\colar{\colag{\Omega} \to \colag{\Comega}}) \longto \copow{\colag \Omega}
	\end{equation*}
	}

	In a single-player game, typically $\colag\Omega$ is `finite', $\colag{\Comega = \R}$ and $\colag\varepsilon$ is $\argmax$:
	\begin{equation*}
		\colag{\argmax(\colar{u : \colag\Omega \to \colag\R})} = \{\colag{\omega \in \Omega} \suchthat \text{$\colag\omega$ maximises $\colar u$}\} \subseteq \colag\Omega.
	\end{equation*}
\end{frame}

% a player observes how other players chose to play in the game (i.e. their strategies), but their only leverage is to change their own strategy.
% a Nash equilibrium is a stand-off situation: nobody can change strategy without getting worse outcomes _given the other players do not deviate_
% notice this doesn't mean that, as a group, players realize their optimal outcome, which might require players to cooperate and deviate jointly
\begin{frame}{What is a Nash equilibrium?}
	\begin{center}
		A strategy profile is a \textbf{Nash equilibrium}\\
		if no player has incentive to \emph{unilaterally} change strategy.
	\end{center}

	\vfill
	\begin{center}
		\includegraphics[width=.7\textwidth]{figures/nash.png}
	\end{center}

	\vfill
	\textcolor{colornote}{Traditionally `the' goal of game theory is determining Nash equilibria of games, though this is not necessarily the case anymore (see: \cite{fudenberg1998theory}).}
\end{frame}

\newcommand{\altunderbrace}[3]{\alt<#1->{\underbrace{#2}_{#3}}{#2}}
\newcommand{\altoverbrace}[3]{\alt<#1->{\overbrace{#2}^{#3}}{#2}}

\begin{frame}{What is a Nash equilibrium?}
	The assignment $\colag{\S(\Omega,\Comega)} := (\colar{\colag{\Omega} \to \colag{\Comega}}) \longto \copow{\colag \Omega}$ is functorial on $\Lens$.

	\vfill
	Crucially, $\colag\S$ admits a lax monoidal structure we call \textbf{Nash product}:
	\begin{eqalign*}
		&\colag{- \boxtimes -} : \colag{\S(\Omega_1,\Comega_1)} \times \colag{\S(\Omega_2, \Comega_2)} \longto \colag{\S(\Omega_1 \times \Omega_2, \Comega_1 \times \Comega_2)}\\[1ex]
		&\colag{(\varepsilon \boxtimes \eta)}(\colar{u}) = \{ \colag{(\omega_1,\omega_2)} \suchthat \colag{\omega_1} \in \colag{\varepsilon}(\colar{u_1(-,\colag{\omega_2})})\ \text{and}\ \colag{\omega_2} \in \colag{\eta}(\colar{u_2(\colag{\omega_1}, -)})\}\\
		&\phantom{\colag{(\varepsilon \boxtimes \eta)}(\colar{u})}\onslide<2->{ = \textcolor{coloraccent}{\text{profiles s.t. `no agent wants to unilaterally deviate'}}}
	\end{eqalign*}
	where $\colar{u = (u_1,u_2) : \colag{\Omega_1} \times \colag{\Omega_2} \to \colag{\Comega_1} \times \colag{\Comega_2}}$.

	\vfill
	\onslide<3->{
		Hence if we have players $\colag{P = \{1, \ldots, N\}}$, each with preferences $\colag{\varepsilon_i \in \S(\Omega_i, \Comega_i)}$, we get:
		\begin{equation*}
			\colag{\varepsilon_1 \boxtimes \cdots \boxtimes \varepsilon_N} : (\altoverbrace{5}{\altunderbrace{4}{\colar{\colag{\Omega_1 \times \cdots \times \Omega_N}}}{\colag{\text{strat. profiles}}} \;\colar{\longto}\; \altunderbrace{4}{\colag{\Comega_1 \times \cdots \times \Comega_N}}{\colag{\text{reward vectors}}}}{\colar{\text{utility functions/\textbf{closed arenas}}}}) \longto \altunderbrace{6}{\copow{(\colag{\Omega_1 \times \cdots \times \Omega_N})}}{\textcolor{coloraccent}{\text{Nash equilibria}}}
		\end{equation*}
	}
\end{frame}

% \begin{frame}{Interlude: selection functions}
% 	Let $\S(X,S) := (X \to S) \to \pow{X}$. This is a functor on lenses:
% 	\begin{eqalign*}
% 		\S(\alpha) : \S(X,S) &\longto \S(Y,R)\\
% 		\varepsilon &\longmapsto \lambda k \,.\, \{x \cmp \alpha \suchthat x \in \varepsilon(\alpha \cmp k)\}
% 	\end{eqalign*}
% 	where $\alpha : (X, S) \to (Y,R)$ and we implicitly used $\Lens((1,1),(Y,R)) \iso Y \to R$ and $\Lens((X,S),(1,1)) \iso X$:

% 	(drawing)

% 	This functor is lax monoidal (\textbf{Nash product}):
% 	\begin{eqalign*}
% 		- \boxtimes - : \S(X,S) \times \S(Y,R) &\longto \S(X \times Y, S \times R)\\
% 		(\varepsilon, \eta) &\longmapsto \lambda k \,.\, \{ (x,y) \suchthat x \in \varepsilon(k_1(x,y))\ \text{and}\ y \in \eta(k_2(x,y))\}
% 	\end{eqalign*}
% 	(drawing)

% 	\textcolor{colornote}{This yields a moncat $\Lens_\S = \underset{mon}\int \S$.}
% \end{frame}

\begin{frame}{The definition}
	\begin{definition}
		An \textbf{open game with agency} is a pair
		\begin{equation*}
			\G \quad = \quad (\ \colar{\A : (X,S) \nlongto{\colag{(\Omega, \Comega)}} (Y,R)}, \quad \colag{\varepsilon  : \S(\Omega, \Comega)}\ )
		\end{equation*}
		whose equilibria are given by
		\begin{equation*}
			\eq_\G(x, k) = \colag{\varepsilon(}\colar{x \cmp \A \cmp k}\colag{)}.
		\end{equation*}%
		\textcolor{colornote}{In this way we recover the equilibrium predicate of open games (\cite{ghani2018compositional}).}
	\end{definition}

	\vfill
	\onslide<2->{
		\begin{definition}
			$\G$ \textbf{has set of players $\colag P$} when
			\begin{equation*}
				\colag{
					\Omega = \prod_{p \in P} \Omega_p, \quad \Comega = \prod_{p \in P} \Comega_p, \quad \varepsilon = \Boxtimes_{p \in P} \varepsilon_p
				}
			\end{equation*}
			in which case
			\begin{equation*}
				\eq_\G(x, k) = \colag{(\Boxtimes_{p \in P} \varepsilon_p)(}\colar{x \cmp \A \cmp k}\colag{)}.
			\end{equation*}
		\end{definition}
	}
	%where $(-)^\top : \Lens_{(\Omega, \Comega)}((1,1),(1,1))\ \iso\ \Omega \to \Comega$.
\end{frame}

% Let's focus on arenas, aka parametrised lenses, for a while...
\begin{frame}{Composing games}
	%\textcolor{colorarena}{Arenas} are the compositional heart of open games with agency:

	\begin{itemize}
		\item \textbf{Sequential composition}
		\begin{center}
			\includegraphics[clip, page=7, trim=2.5cm 21cm 1.5cm 1cm, width=.9\textwidth]{figures/drawings.pdf}
		\end{center}
		\item \textbf{Parallel composition}
		\begin{center}
			\includegraphics[clip, page=7, trim=2cm 5cm 1cm 13cm, width=.9\textwidth]{figures/drawings.pdf}
		\end{center}
	\end{itemize}

	\textbf{Notice}: these operators \underline{can} be extended to games:
	\begin{equation*}
		(\colar{\A_1}, \colag\varepsilon) \cmp (\colar{\A_2}, \colag\eta) := (\colar{\A_1 \cmp \A_2}, \colag{\varepsilon \boxtimes \eta}), \quad (\colar{\A_1}, \colag\varepsilon) \otimes (\colar{\A_2}, \colag\eta) := (\colar{\A_1 \otimes \A_2}, \colag{\varepsilon \boxtimes \eta})
	\end{equation*}
\end{frame}

\begin{frame}{Composing arenas}

	\begin{itemize}
		\item \textbf{External choice}
		\begin{center}
			\includegraphics[clip, page=8, trim=0 17cm 2cm 4cm, width=.9\textwidth]{figures/drawings.pdf}
		\end{center}
		The `environment' chooses which game to play, agents are prepared to play both.
		\item \textbf{Internal choice}
		\begin{center}
			\includegraphics[clip, page=8, trim=1.5cm 7cm 2cm 14cm, width=.9\textwidth]{figures/drawings.pdf}
		\end{center}
		The `environment' can play either game, agents choose which one.
	\end{itemize}

	\textbf{Notice}: these operators \underline{can't} be extended to selection functions in a canonical way! \textcolor{colornote}{(Actually $\oplus$ \underline{can} if we refine our typing judgments)}
\end{frame}

\begin{frame}{Reparametrisation}
	Most importantly arenas form a (locally fibred) \textbf{bicategory}: one can \textbf{reparametrise} along a lens $\colag{\alpha : (\Omega', \Comega') \to (\Omega, \Comega)}$.

	\vfill
	\begin{center}
		\includegraphics[clip, page=9, trim=0cm 19cm 13cm 0cm, width=.9\textwidth]{figures/drawings.pdf}
	\end{center}

	\vfill
	This is crucial for introducing agency!

	\vfill
	Lenses in the blue direction represent `\textcolor{coloragents}{players dynamics}',\\
	\hspace{5ex}e.g. voting, observational constraints (imperfect information),\\
	\hspace{5ex}\phantom{e.g. }rewards distribution (imputation), ...
	%\vfill
	%\textcolor{colornote}{If we work in $\Para_{\times_\S}(\Lens_\S)$, we see that `$(1,1,\top)$ coclassifies equilibria', i.e. $(1,1,\top) \twoto \G \iso \eq_\G$.}
\end{frame}

\begin{frame}{Regrouping}
	If $\colar\A$ has set of players $\colag P$ and $\colag{r : P \to Q}$ is a function, we can turn $\colar\A$ into an arena with players $\colag Q$ by reparametrising along the permutation of $\colag{\prod_{p \in P} \Omega_P}$ induced by $\colag r$:
	\begin{equation*}
		\colag{\regroup_r : ({\textstyle\prod}_{q \in Q} (r^* \Omega)_q, {\textstyle\prod}_{q \in Q} (r^* \Comega)_q) \longto ({\textstyle\prod}_{p \in P} \Omega_p, {\textstyle\prod}_{p \in P} \Comega_p)}
	\end{equation*}
	\begin{example}<2->
		If $\colar{\A_1}$ and $\colar{\A_2}$ have the same players $\colag{P = \{1,2\}}$, $\colar{\A_1 \cmp \A_2}$ has players $\colag{P+P}$. Regrouping along $\colag{\nabla : P+P \to P}$ restores the correct set of players.
		\begin{center}
			\only<-2>{
				\includegraphics[clip, page=10, trim=0cm 24.5cm 25.7cm .5cm, width=.7\textwidth]{figures/drawings.pdf}
			}
			\only<3>{
				\includegraphics[clip, page=10, trim=18cm 13cm 7cm 6.6cm, width=.7\textwidth]{figures/drawings.pdf}
			}
			\only<4>{
				\includegraphics[clip, page=10, trim=0cm 13cm 25cm 7cm, width=.7\textwidth]{figures/drawings.pdf}
			}
			\only<5>{
				\includegraphics[clip, page=10, trim=.5cm 2.2cm 25cm 17.5cm, width=.7\textwidth]{figures/drawings.pdf}
			}
		\end{center}
	\end{example}
\end{frame}

\begin{frame}{Tying}
	By reparametrising along $\colag{(\Delta, \mathsf{combine})}$ (where $\colag{\mathsf{combine} = +, \pi_2, \max, \ldots}$), we can enforce the same strategies to be played at different points of a game.

	\begin{example}
		Repeated games: play the same strategies at every round
		\begin{center}
			\includegraphics[clip, page=11, trim=0cm 15cm 10cm 0cm, width=.85\textwidth]{figures/drawings.pdf}
		\end{center}
	\end{example}

	\vfill
	\onslide<2->{
		\textbf{Notice}: $\colag{(\Delta, \mathsf{combine})^*}\colar{(\A_1 \cmp \A_2)}$ lies outside the image of $\colar\cmp$/$\colar\otimes$/$\colar\oplus$/$\colar\&$, hence introduces 'non-compositional' effects \hspace{3ex}$\rightsquigarrow$ \textbf{`\textcolor{coloragents}{agency} is non-local'.}
	}
	%\textcolor{colornote}{We'll use this soon to treat imperfect information. Also useful for Markov games.}
\end{frame}
